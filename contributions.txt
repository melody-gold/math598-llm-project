Contributions Document:
I just listed the blocks I contributed to but want to note that what I put down are not things I did by myself, just the things I recall adding to/creating. 


Talia Kumar's Contributions:
    Getting Data Block:
        - Helped find the data and clean the text on import
    Tokenize Text Block:
        - Helped create the init function and the clean text function
        - Provided (mostly verbal/collaborative) help on the tokenizer and detokenizer
    Attention Head Block:
        - Created the M_matrix function
    Transformer Block Block (lol):
        - Helped create the init and forward functions
    Save and Load Modules Block:
        - Created Save and Load modules functions
    Py Testing:
        - Created test github action workflow and the first few tests

Jordyn Graham's Contribution: 
    Note: I did miss the first day of working on this project and joined the 
    group late on the second work day in class. 
    1. Tokenizer class 
        - I primarily worked on the vocab map, encoder, and decoder. To test it, I set up 
          a test case using the raw data. 
        - In class I worked collaboratively on this block as to the steps and approach. 
    2. MLP class
        - I worked collaboratively on the forward function and overall architecture. Did not 
          write code but instead provided verbal comments and help.  
    3. Transformer Block
         - Worked on in class and did not write any code but instead provided verbal collaboration. 
    4. Training Loop 
         - Created training loop function to take in tokenized text and then split it for training 
           and validation tests. 