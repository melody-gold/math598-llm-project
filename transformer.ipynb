{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass \n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import requests\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class Config:\n",
    "    d_model : int \n",
    "    d_vocab : int\n",
    "    d_hidden : int # for MLP\n",
    "    n_context_max : int # important for training loop (max \"slice\" size)\n",
    "    n_context: int \n",
    "    n_layers : int\n",
    "    \n",
    "    # d_head : int # for Attn (if separate wq and wk)\n",
    "    #no n_context\n",
    "    #name var : type\n",
    "\n",
    "# guttenburg dataset code in existing notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c76b0",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\text{d-model} = d_m & : & \\text{model dimension (num neurons)} \\\\\n",
    "    &d_v = \\text{d-vocab} & : & \\text{vocab dimension} \\\\\n",
    "    &n_c = \\text{n-context} & : & \\text{context window (len of seq entered)}\n",
    "\\end{align*}\n",
    "\n",
    "Where $d_n << d_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf9463",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/67098/67098-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "start_idx = text.find(start_marker)\n",
    "end_idx = text.find(end_marker)\n",
    "content_area = text[start_idx:end_idx].split(\"\\n\", 1)[1]\n",
    "\n",
    "chapter_idx = content_area.upper().find(\"CHAPTER I\")\n",
    "\n",
    "raw_text = content_area[chapter_idx:]\n",
    "print(raw_text[:500000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69d411",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self, text):\n",
    "        # clean and sort data \n",
    "        cleaned = self.clean_text(text)\n",
    "        self.chars = sorted(set(cleaned))\n",
    "        # vocab size \n",
    "        self.vocab_size = len(self.chars)\n",
    "\n",
    "        # vocab map \n",
    "        self.encode = {}\n",
    "        self.decode = {}\n",
    "\n",
    "        for i, chars in enumerate(self.chars):\n",
    "            self.encode[chars] = i\n",
    "            self.decode[i] = chars\n",
    "\n",
    "        # sanity check \n",
    "        print(f\"Vocab chars:\" ,self.chars)\n",
    "        print(f\"Vocab size:\" , self.vocab_size)\n",
    "\n",
    "    #simplify\n",
    "    #all letters lowercase\n",
    "    #each punctuation into a token each letter a token\n",
    "    #get a set of tokens\n",
    "    #this set is d_vocab\n",
    "\n",
    "    # clean the text \n",
    "    def clean_text(self, text: str) -> list[str]:\n",
    "        return [x for x in text.lower() if x.isalpha() or x in \" .!?\"]\n",
    "\n",
    "    # encoder \n",
    "    def tokenize(self, text):\n",
    "    # will update words to nums with vocab map \n",
    "        cleaned = self.clean_text(text)\n",
    "\n",
    "        tokens = []\n",
    "        \n",
    "        for char in cleaned:\n",
    "            if char in self.encode:\n",
    "                tokens.append(self.encode[char])\n",
    "\n",
    "        print(\"Encoded tokens:\", tokens)\n",
    "        return tokens\n",
    "\n",
    "    # decoder \n",
    "    def detokenize(self, tokens):\n",
    "    #inverse of tokenize (nums to words )\n",
    "        words = []\n",
    "        for id in tokens:\n",
    "            if id in self.decode:\n",
    "                words.append(self.decode[id])\n",
    "\n",
    "        final = \"\".join(words)\n",
    "        print(\"Decoded text:\", final)\n",
    "        \n",
    "        return final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case for tokenizer class \n",
    "tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "sample_text = \"Chapter I. The starting of the journey!\"\n",
    "\n",
    "encoded = tokenizer.tokenize(sample_text)\n",
    "decoded = tokenizer.detokenize(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b953f1",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "$$\n",
    "    \\texttt{MLP}(\\mathbf{X}) = W_d \\cdot \\sigma_{\\texttt{ReLU}} (W_u \\cdot x + b_u) + b_d, \\qquad \\texttt{MLP} : \\mathbb{R}^{d_m} \\to \\mathbb{R}^{d_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59db994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config): # matrices to initialize\n",
    "        super().__init__()\n",
    "        self.linear_up: nn.Linear = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear_down: nn.Linear = nn.Linear(config.d_hidden, config.d_model)\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        x = self.linear_up(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_down(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b6b7f",
   "metadata": {},
   "source": [
    "## Attention Head\n",
    "### Weight Matrix\n",
    "$$\n",
    "    \\mathbf{W}_{QK} := \\mathbf{W}_{Q} \\cdot \\mathbf{W}_{K}^T, \\qquad \\mathbf{W}_{Q}, \\mathbf{W}_{K} \\in \\mathbb{R}^{d_m \\times d_n}\n",
    "$$\n",
    "\n",
    "### Autoregressive Masking (M) Matrix\n",
    "$$\n",
    "    M_{i,j} = \n",
    "    \\begin{cases}\n",
    "        0 &j \\geq i \\\\\n",
    "        -\\infty &j < i>\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "### Forward Pass\n",
    "$$A(\\mathbf{X}) = \\sigma_{\\text{softmax}} (\\mathbf{X} \\; \\mathbf{W}_{QK} \\; \\mathbf{X}^\\text{T} + \\mathbf{M}) \\; \\mathbf{X} \\; \\mathbf{W}_{OV}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c587bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # weights (use nn.parameter) to create a matrix to track gradients\n",
    "        self.wqk = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "        self.wov = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "\n",
    "        ## Create M Matrix\n",
    "    def M_matrix(self, n):\n",
    "        # matrix with 0 at and below the diagonal and -inf above the diagonal\n",
    "        M = torch.ones((n, n))\n",
    "        M = torch.triu(M, diagonal=1)\n",
    "        M = M.masked_fill(M == 1, float('-inf'))\n",
    "        print(M)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        # use weights to compute Aâ¨‰\n",
    "        # X as input: n_seq by d_model\n",
    "        n_seq = x.shape[0]\n",
    "        M = self.M_matrix(n_seq)\n",
    "        attention_pattern = x @ self.wqk @ x.T + M\n",
    "        attention_of_X = nn.Softmax(attention_pattern) @ x @ self.wov\n",
    "        \n",
    "        return attention_of_X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e36789",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "$$\n",
    "    \\text{TB}(X) = X + A(X) + \\text{MLP}(X), \\qquad \\text{TB}: \\mathbb{R}^{n_c \\times d_m} \\to \\mathbb{R}^{n_c \\times d_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37000370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        #self.ln = nn.LayerNorm(config)\n",
    "        self.mlp = MLP(config)\n",
    "        self.attention = AttentionHead(config)\n",
    "\n",
    "\n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        #output = x + mlp(x) + attentionhead(x)   \n",
    "        output_x = x + self.mlp(x) + self.attention(x)\n",
    "        #x = self.ln(x_1)\n",
    "        #x = self.ln(x_2)\n",
    "\n",
    "        return output_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa29c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.pos_embedding = nn.Embedding(config.n_context_max, config.d_model)\n",
    "        #self.transformerblocks = nn.modules list of transformer blocks\n",
    "        self.transformerblocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "    def forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "        x = self.token_embedding(x) # converts int d-vector to d-model vector\n",
    "        x = x + self.pos_embedding(torch.arange(x.shape[0])) # x = E + P\n",
    "        # pos_embedding(x) uses nn.Embedding of torch.arrange(n_context)\n",
    "        for i in range(self.config.n_layers):\n",
    "            x = self.transformerblocks[i](x)\n",
    "        x = x @ self.token_embedding.weight.T # unembedding \n",
    "        #n_contex long - sequence if ints of length n  - float tneosry by n_model  and output is float tencsosr by d-vocab \\n\",\n",
    "        #d_model to d_vocab transpose or do a lineear map  - unembed nn.linear\n",
    "        #dmodel to dvocab \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def generator(self, num_tokens = 10, input_text = \"\"):# some text, number of new token, and return esseuquence of text - tokenzise text, sequence of numbers, numbers in model and get probaility, sample probablities, detonize \n",
    "    \n",
    "        tokenizer = Tokenizer(raw_text)\n",
    "        tokenized_text =  tokenizer.tokenize(input_text)\n",
    "        input_tensor = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "        for i in range(num_tokens):\n",
    "                out = self.forward(input_tensor)\n",
    "                print(\"Finished running through forward!\")\n",
    "                probailities = torch.softmax(out[:, -1], dim = -1)\n",
    "                new_token = torch.multinomial(probailities, num_samples= 1)\n",
    "                new_input_tensor = torch.cat([input_tensor, new_token], dim = -1)\n",
    "                input_tensor = new_input_tensor\n",
    "        detokenized_text = tokenizer.detokenize(input_tensor.tolist())\n",
    "\n",
    "        return detokenized_text\n",
    "   \n",
    "# use nn.ModuleList for TB seqeunce & MHA (to create a list of TBS)\n",
    "# print(f\"{x.shape = }\") for debugging\n",
    "\n",
    "# pick a unique dataset to train data on\n",
    "\n",
    "# if traning models: aim for < 10 million parameters for now\n",
    "#   sum(x.numel() for x in mymodel.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate function based on user input for n_layers : int, d_model : int, d_vocab : int, d_hidden : int\n",
    "# def Generator(User_n_layers : int, User_d_model : int, User_d_vocab : int, User_d_hidden : int):\n",
    "#     configuration = Config(User_n_layers, User_d_model, User_d_vocab, user_d_hidden)\n",
    "#     Tranformer_model = transformer(configuration)\n",
    "#     return Tranformer_model\n",
    "\n",
    "#model_initialize_something "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303b48f",
   "metadata": {},
   "source": [
    "# Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, tokens, config, path=\"my_first_transformer.pt\"):\n",
    "    torch.save({\n",
    "        \"model_state\": model.state_dict(),\n",
    "        \"config\": config,                   \n",
    "        \"vocab\": tokenizer.vocab,          \n",
    "    }, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(path=\"my_first_transformer.pt\"):\n",
    "    load_in = torch.load(path)\n",
    "    config = load_in[\"config\"]\n",
    "    model_loaded = transformer(config)\n",
    "\n",
    "    tokenizer = Tokenizer(None)\n",
    "    tokenizer.vocab = load_in[\"vocab\"]\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    return model_loaded, tokenizer, config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Book_Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, raw_text, train_len):\n",
    "        self.tokenizer = Tokenizer(raw_text)\n",
    "        self.tokens = torch.tensor(self.tokenizer.tokenize(raw_text), dtype=torch.long)\n",
    "        self.n_context = train_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens) - self.n_context\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.tokens[index:index + self.n_context]\n",
    "        y = self.tokens[index + 1:index + self.n_context + 1]\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa02f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop \n",
    "\n",
    "def train_loop(samples, batchsize, model, epochs):\n",
    "\n",
    "    # wrap an iterable to enable easy access to samples \n",
    "    data_loader = DataLoader(samples, batch_size = batchsize, shuffle = True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # if don't need to do a split then use: \n",
    "    print(\"<<<< Training Started >>>>\")\n",
    "    \n",
    "    model.train()\n",
    "    loss_train = 0 \n",
    "    for x, y in data_loader: \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        if y.ndim > 1:\n",
    "            y = torch.argmax(y, dim =1)\n",
    "        y = y.long()\n",
    "\n",
    "        batch, training_len, vocab = output.shape\n",
    "\n",
    "        loss = criterion(output.view(batch * training_len, vocab), y.view(batch * training_len))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        loss_train += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {i +1}: Loss = {loss_train:.4f}\")\n",
    "\n",
    "    print(\"<<<< Training Complete >>>>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e52b08",
   "metadata": {},
   "source": [
    "### Implement Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134107a",
   "metadata": {},
   "source": [
    "#### Already ran at beginning of code\n",
    "1) import raw text \n",
    "2) tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "\n",
    "#### Now: \n",
    "Set up config, run tokenized text through dataset (update to tensor), train loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(d_model=64, d_vocab=31, d_hidden=128, n_layers=2, n_context=64)\n",
    "model = transformer(cfg)\n",
    "samples = Book_Dataset(raw_text, cfg.n_context)\n",
    "train_loop(samples, batchsize=32, model=model, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(samples, batchsize= 32, model=transformer, epochs = 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d55c7",
   "metadata": {},
   "source": [
    "Initialize model:\n",
    "1. Training loop on model and generation on model.\n",
    "2. (model_stupid) no training loop, generation on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6564cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0ec8e4",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "(Using markov chains?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b8007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding-gpt (3.11.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
