{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b372e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from dataclasses import dataclass \n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from jaxtyping import Float, Int\n",
    "\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "\n",
    "\n",
    "@dataclass \n",
    "class Config:\n",
    "    d_model : int \n",
    "    d_vocab : int\n",
    "    d_hidden : int\n",
    "    #no n_context\n",
    "    #name var : type\n",
    "\n",
    "# guttenburg dataset code in existing notebooks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c76b0",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gutenberg_book(\n",
    "\tid: int|None = 84,\n",
    "\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\tremove_gutenberg_meta: bool = True,\n",
    ") -> str:\n",
    "\t\n",
    "\tdata_temp = Path(data_temp)\n",
    "\tdata_temp.mkdir(parents=True, exist_ok=True)\n",
    "\t\n",
    "\turl: str = f\"https://www.gutenberg.org/cache/epub/{id}/pg{id}.txt\"\n",
    "\tdata_path: Path = Path(data_temp) / f\"{id}.txt\"\n",
    "\tdata: str\n",
    "\t# read from cache if it exists\n",
    "\tif data_path.exists():\n",
    "\t\twith open(data_path, 'r', encoding='utf-8') as file:\n",
    "\t\t\tdata = file.read()\n",
    "\telse:\n",
    "\t\t# download if it doesn't exist\n",
    "\t\tresponse = requests.get(url)\n",
    "\t\tresponse.raise_for_status()  # Ensure that the download was successful\n",
    "\t\tdata = response.text\n",
    "\n",
    "\t\t# save to cache\n",
    "\t\twith open(data_path, 'w', encoding='utf-8') as file:\n",
    "\t\t\tfile.write(data)\n",
    "\n",
    "\t# remove header/footer\n",
    "\tif remove_gutenberg_meta:\n",
    "\t\tdata = '***'.join(data.split('***')[2:])\n",
    "\t\tdata = '***'.join(data.split('***')[:-1])\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "def get_many_books(\n",
    "\t\tids: list[int],\n",
    "\t\tdata_temp: Path|str = \"../data/gutenberg_data\",\n",
    "\t) -> list[str]:\n",
    "\t\n",
    "\tdata: list[str] = []\n",
    "\tfor id in ids:\n",
    "\t\tprint(f\"Getting book {id}...\")\n",
    "\t\titem: str = get_gutenberg_book(id, data_temp)\n",
    "\t\tprint(f\"\\t{len(item)} characters read\")\n",
    "\t\tdata.append(item)\n",
    "\t\n",
    "\treturn data\n",
    "\n",
    "DATA_RAW: list[str] = get_many_books([84, 15, 18, 82, 996, 2600])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b953f1",
   "metadata": {},
   "source": [
    "$$\n",
    "    \\texttt{MLP}(\\mathbf{X}) = W_d \\cdot \\sigma_{\\texttt{ReLU}} (W_u \\cdot x + b_u) + b_d\n",
    "    $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59db994",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, config: Config): # matrices to initialize\n",
    "        super().__init__()\n",
    "        self.linear_up: nn.Linear = nn.Linear(config.d_model, config.d_hidden)\n",
    "        self.linear_down: nn.Linear = nn.Linear(config.d_hidden, config.d_model)\n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        x = self.linear_up(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.linear_down(x)\n",
    "        return x  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b6b7f",
   "metadata": {},
   "source": [
    "## Attention Head\n",
    "### Weight Matrix\n",
    "$$\n",
    "    \\mathbf{W}_{QK} := \\mathbf{W}_{Q} \\cdot \\mathbf{W}_{K}^T\n",
    "$$\n",
    "\n",
    "### Forward Pass\n",
    "$$A(\\mathbf{X}) = \\sigma_{\\text{softmax}} (\\mathbf{X} \\; \\mathbf{W}_{QK} \\; \\mathbf{X}^\\text{T} + \\mathbf{M}) \\; \\mathbf{X} \\; \\mathbf{W}_{OV}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c587bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        # weights (use nn.parameter) to create a matrix to track gradients\n",
    "        self.wqk = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "        self.wov = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "\n",
    "        ## Create M Matrix\n",
    "    def M_matrix(n):\n",
    "        # matrix with 0 at and below the diagonal and -inf above the diagonal\n",
    "        M = torch.ones((n, n))\n",
    "        M = torch.triu(M, diagonal=1)\n",
    "        M = M.masked_fill(M == 1, float('-inf'))\n",
    "        print(M)\n",
    "        \n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        # use weights to compute A\n",
    "        # at X as input: n_seq by d_model\n",
    "        n_seq = x.shape[0]\n",
    "        # M = m_matrix of context window size\n",
    "        M = self.M_matrix(n_seq)\n",
    "        # \n",
    "        \n",
    "        # @ for matrix multiplication\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e36789",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37000370",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "    \n",
    "    # mlp and attention head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa29c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decoding-gpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
