{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb7bdc5f",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5df7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from typing import List\n",
    "import unicodedata\n",
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import base64\n",
    "from IPython.display import Image, display\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# Import from local modules\n",
    "from config import Config\n",
    "from tokenizer import Tokenizer\n",
    "from model import MLP, AttentionHead, TransformerBlock, transformer\n",
    "from dataset import Book_Dataset\n",
    "from train import train_loop, save_model, load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f6c76b0",
   "metadata": {},
   "source": [
    "### Dimensions\n",
    "\n",
    "\\begin{align*}\n",
    "    &\\text{d-model} = d_m & : & \\text{model dimension (num neurons)} \\\\\n",
    "    &d_v = \\text{d-vocab} & : & \\text{vocab dimension} \\\\\n",
    "    &n_c = \\text{n-context} & : & \\text{context window (len of seq entered)}\n",
    "\\end{align*}\n",
    "\n",
    "Where $d_n << d_m$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53bf9463",
   "metadata": {},
   "source": [
    "## Getting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dccfcc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://www.gutenberg.org/files/67098/67098-0.txt\"\n",
    "response = requests.get(url)\n",
    "text = response.text\n",
    "\n",
    "start_marker = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
    "end_marker = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
    "\n",
    "start_idx = text.find(start_marker)\n",
    "end_idx = text.find(end_marker)\n",
    "content_area = text[start_idx:end_idx].split(\"\\n\", 1)[1]\n",
    "\n",
    "chapter_idx = content_area.upper().find(\"CHAPTER I\")\n",
    "\n",
    "raw_text = content_area[chapter_idx:]\n",
    "print(raw_text[:500000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b69d411",
   "metadata": {},
   "source": [
    "## Tokenize Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9783922c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizer class is defined in tokenizer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aadd42de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test case for tokenizer class \n",
    "tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "sample_text = \"Chapter I. The starting of the journey!\"\n",
    "\n",
    "encoded = tokenizer.tokenize(sample_text)\n",
    "decoded = tokenizer.detokenize(encoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b953f1",
   "metadata": {},
   "source": [
    "## Multilayer Perceptron\n",
    "$$\n",
    "    \\texttt{MLP}(\\mathbf{X}) = W_d \\cdot \\sigma_{\\texttt{ReLU}} (W_u \\cdot x + b_u) + b_d, \\qquad \\texttt{MLP} : \\mathbb{R}^{d_m} \\to \\mathbb{R}^{d_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59db994",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP class is defined in model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d56b6b7f",
   "metadata": {},
   "source": [
    "## Attention Head\n",
    "### Weight Matrix\n",
    "$$\n",
    "    \\mathbf{W}_{QK} := \\mathbf{W}_{Q} \\cdot \\mathbf{W}_{K}^T, \\qquad \\mathbf{W}_{Q}, \\mathbf{W}_{K} \\in \\mathbb{R}^{d_m \\times d_n}\n",
    "$$\n",
    "\n",
    "### Autoregressive Masking (M) Matrix\n",
    "$$\n",
    "    M_{i,j} = \n",
    "    \\begin{cases}\n",
    "        0 &j \\geq i \\\\\n",
    "        -\\infty &j < i>\n",
    "    \\end{cases}\n",
    "$$\n",
    "\n",
    "### Forward Pass\n",
    "$$A(\\mathbf{X}) = \\sigma_{\\text{softmax}} (\\mathbf{X} \\; \\mathbf{W}_{QK} \\; \\mathbf{X}^\\text{T} + \\mathbf{M}) \\; \\mathbf{X} \\; \\mathbf{W}_{OV}$$\n",
    "\n",
    "For calculating weights, use `nn.Parameter` to create a matrix that tracks gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c587bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.wqk = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "        self.wov = nn.Parameter(torch.randn(config.d_model, config.d_model))\n",
    "\n",
    "    # Create M Matrix\n",
    "    def M_matrix(self, n):\n",
    "        # matrix with 0 at and below the diagonal and -inf above the diagonal\n",
    "        M = torch.ones((n, n))\n",
    "        M = torch.triu(M, diagonal=1)\n",
    "        M = M.masked_fill(M == 1, float('-inf'))\n",
    "        return M\n",
    "        \n",
    "    \n",
    "    def forward(self, x: Float[torch.Tensor, \"* d_model\"]) -> Float[torch.Tensor, \"* d_model\"]:\n",
    "        # X: [n_seq by d_model] or [batch_size by n_seq by d_model]\n",
    "        n_seq = x.shape[1]\n",
    "        M = self.M_matrix(n_seq)\n",
    "        attn_pattern = x @ self.wqk @ x.transpose(-1, -2) + M\n",
    "        attn = torch.softmax(attn_pattern, dim=-1) @ x @ self.wov\n",
    "        \n",
    "        return attn"
    "# AttentionHead class is defined in model.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e36789",
   "metadata": {},
   "source": [
    "### Transformer Block\n",
    "$$\n",
    "    \\text{TB}(X) = X + A(X) + \\text{MLP}(X), \\qquad \\text{TB}: \\mathbb{R}^{n_c \\times d_m} \\to \\mathbb{R}^{n_c \\times d_m}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37000370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerBlock class is defined in model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa29c7a",
   "metadata": {},
   "outputs": [],
   "source": [

    "class transformer(nn.Module):\n",
    "    def __init__(self, config: Config):\n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.token_embedding = nn.Embedding(config.d_vocab, config.d_model)\n",
    "        self.pos_embedding = nn.Embedding(config.n_context_max, config.d_model)\n",
    "        #self.transformerblocks = nn.modules list of transformer blocks\n",
    "        self.transformerblocks = nn.ModuleList([TransformerBlock(config) for _ in range(config.n_layers)])\n",
    "        \n",
    "    def forward(self, x: Int[torch.Tensor, \"n_context\"]) -> Float[torch.Tensor, \"n_context d_vocab\"]:\n",
    "        x = self.token_embedding(x) # converts int d-vector to d-model vector\n",
    "            # x.shape[0] = tuple of dim sizes, x.shape[1] size of 1st dim (n_context)\n",
    "            # x.device = device of x\n",
    "        x = x + self.pos_embedding(torch.arange(x.shape[1], device=x.device)) # x = E + P\n",
    "        for i in range(self.config.n_layers):\n",
    "            x = self.transformerblocks[i](x)\n",
    "        x = x @ self.token_embedding.weight.T # unembedding \n",
    "        # n_context long - sequence if ints of length n  - float tneosr by n_model and output is float tensor by d-vocab \\n\",\n",
    "        # d_model to d_vocab transpose or do a lineear map  - unembed nn.linear\n",
    "        # d_model to dvocab \n",
    "\n",
    "        return x\n",
    "    \n",
    "    def generator(self, num_tokens = 10, input_text = \"\"):\n",
    "        # some text, number of new token, and return esseuquence of text - \n",
    "        # tokenzise text, sequence of numbers, numbers in model and get probaility, sample probablities, detonize \n",
    "        tokenizer = Tokenizer(raw_text)\n",
    "        tokenized_text =  tokenizer.tokenize(input_text)\n",
    "        input_tensor = torch.tensor(tokenized_text, dtype=torch.long)\n",
    "        for i in range(num_tokens):\n",
    "            out = self.forward(input_tensor)\n",
    "            print(\"Finished running through forward!\")\n",
    "            probailities = torch.softmax(out[:, -1], dim = -1)\n",
    "            new_token = torch.multinomial(probailities, num_samples= 1)\n",
    "            new_input_tensor = torch.cat([input_tensor, new_token], dim = -1)\n",
    "            input_tensor = new_input_tensor\n",
    "        detokenized_text = tokenizer.detokenize(input_tensor.tolist())\n",
    "\n",
    "        return detokenized_text\n",
    "   \n",
    "# use nn.ModuleList for TB seqeunce & MHA (to create a list of TBS)\n",
    "# print(f\"{x.shape = }\") for debugging\n",
    "\n",
    "# pick a unique dataset to train data on\n",
    "\n",
    "# if traning models: aim for < 10 million parameters for now\n",
    "#   sum(x.numel() for x in mymodel.parameters())\n"
    "# transformer is defined in model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df8e29f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate function based on user input for n_layers : int, d_model : int, d_vocab : int, d_hidden : int\n",
    "# def Generator(User_n_layers : int, User_d_model : int, User_d_vocab : int, User_d_hidden : int):\n",
    "#     configuration = Config(User_n_layers, User_d_model, User_d_vocab, user_d_hidden)\n",
    "#     Tranformer_model = transformer(configuration)\n",
    "#     return Tranformer_model\n",
    "\n",
    "#model_initialize_something "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e303b48f",
   "metadata": {},
   "source": [
    "# Save and Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187f5a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_model and load_model are defined in train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598c80e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Book_Dataset class is defined in dataset.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa02f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training loop \n",
    "def train_loop(samples, batchsize, model):\n",
    "    # wrap an iterable to enable easy access to samples \n",
    "    data_loader = DataLoader(samples, batch_size = batchsize, shuffle = True)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    # if don't need to do a split then use: \n",
    "    print(\"<<<< Training Started >>>>\")\n",
    "    model.train()\n",
    "    loss_train = 0.0\n",
    "    num_batches = len(data_loader)\n",
    "    for batch_num, (x, y) in enumerate(data_loader, start=1):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        y = y.long()\n",
    "\n",
    "        batch, training_len, vocab = output.shape\n",
    "        loss = criterion(output.reshape(batch * training_len, vocab), y.reshape(batch * training_len))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_train += loss.item()\n",
    "        \n",
    "        if batch_num == 1 or batch_num % 100 == 0 or batch_num == num_batches:\n",
    "            print(f\"Batch {batch_num}/{num_batches}: Loss = {loss.item():.4f}, Running Avg = {loss_train/batch_num:.4f}\")\n",
    "\n",
    "    print(\"<<<< Training Complete >>>>\")\n",
    "    print(f\"Final avg loss: {loss_train/len(data_loader):.4f}\")\n"
    " # train_loop is defined in train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e52b08",
   "metadata": {},
   "source": [
    "### Implement Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3134107a",
   "metadata": {},
   "source": [
    "#### Already ran at beginning of code\n",
    "1) import raw text \n",
    "2) tokenizer = Tokenizer(raw_text)\n",
    "\n",
    "\n",
    "#### Now: \n",
    "Set up config, run tokenized text through dataset (update to tensor), train loop  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5d8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = Config(d_model=64, d_vocab=31, d_hidden=128, n_layers=2, n_context=64, n_context_max = 64) # 64 or 128\n",
    "model = transformer(cfg)\n",
    "samples = Book_Dataset(raw_text, cfg.n_context)\n",
    "train_loop(samples, batchsize=32, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b1a186",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop(samples, batchsize= 32, model=transformer, epochs = 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582d55c7",
   "metadata": {},
   "source": [
    "Initialize model:\n",
    "1. Training loop on model and generation on model.\n",
    "2. (model_stupid) no training loop, generation on model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6564cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8d0ec8e4",
   "metadata": {},
   "source": [
    "# Statistical Analysis\n",
    "(Using markov chains?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287b8007",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
